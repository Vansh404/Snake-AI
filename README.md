# Snake-AI

A feedforward neural network is used with three hidden layers.The NN employs  ğ‘…ğ‘’ğ¿ğ‘¢  activation function for its layers. The input layer consists of 11 nodes from the state of the snake, the output layer consists the three action nodes that the snake takes,i.e. the direction it can move in.

The  ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘   are the choices made by the agent
The  ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘   are the basis for making the choices
The  ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘   are the basis for evaluating the choices

Deep Q Algorithm-

1) Initialise Q value /n
2) Choose action to be performed, the action selection policy is e-greedy
3) Perform action(ğ´ğ‘›) for the time step ğ‘› and measure the award (ğ‘…ğ‘›) associated with that action
4) Update the Q value for the action ğ´ğ‘›

Each rule that dicates how actions are done as function of state are called ğ‘ğ‘œğ‘™ğ‘–ğ‘ğ‘–ğ‘’ğ‘ . Each policy has a ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘› which associates every action-state pair to an expected return, if that state-action pair is performed.
An ğ‘œğ‘ğ‘¡ğ‘–ğ‘šğ‘ğ‘™ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘› assigns the largest expected return to each state, or state-action pair. We will be using the Bellman optimality function here to derive these optimal value functions

The 11 states that we will use are [direction left, direction right, direction up, direction down], [food up, food down, food right, food left], [danger straight,danger right, danger left]. The moves are choosen by an ğ‘‘ğ‘’ğ‘ğ‘ğ‘¦ğ‘–ğ‘›ğ‘” ğ‘’ğ‘ğ‘ ğ‘–ğ‘™ğ‘œğ‘› ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ algorithm.

![Screenshot 2022-07-15 013633](https://user-images.githubusercontent.com/79185485/179074108-3114b85e-302b-4c4a-9251-cace0a1635ac.png)
